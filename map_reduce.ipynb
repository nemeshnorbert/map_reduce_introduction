{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы коротко обсудим что представляет из себя технология MapReduce и разберем пару примеров ее использования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Мотивировка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что к вам пришел менеджер со следующей задачей: надо уметь определять сколько каких слов встречается в отзывах к ресторанам. На вход вы получаете текст отзыва, на выходе словарь, где для каждого слова указано количество его повоторений. Вы быстро реализовали прототип на питоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def extract_words(text):\n",
    "    delimiters = [' ', '.', '?', '!', ':', ',', '-', '\"']\n",
    "    regex_pattern = '|'.join(map(re.escape, delimiters))\n",
    "    for word in re.split(regex_pattern, text):\n",
    "        if word:\n",
    "            yield word.lower().strip()\n",
    "            \n",
    "\n",
    "def word_counts(text):\n",
    "    return collections.Counter(extract_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 2,\n",
       "         'food': 1,\n",
       "         'is': 1,\n",
       "         'no': 1,\n",
       "         'doubt': 1,\n",
       "         'superb': 1,\n",
       "         'especially': 1,\n",
       "         'wagyu': 1,\n",
       "         'staff': 1,\n",
       "         'were': 2,\n",
       "         'very': 1,\n",
       "         'attentive': 1,\n",
       "         'i': 1,\n",
       "         'have': 1,\n",
       "         'to': 1,\n",
       "         'say': 1,\n",
       "         'we': 1,\n",
       "         'really': 1,\n",
       "         'impressed': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts(\n",
    "    ('The food is no doubt superb, especially the wagyu. Staff were very attentive.' \n",
    "    'I have to say we were really impressed.')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея с подсчетом оказалась очень удачной - ваш бизнес растет как грибы после дождя. Было решено адаптировать ее к анализу частот слов в запросах пользователей вашего интернет магазина. Начальство оптимистично смотрит в будущее и считает, что скоро нужно будет обрабатывать миллионы запросов в день. Вам же осталось всего лишь адаптировать прототип. Только есть одна проблема - теперь надо обарабатывать не несколько сотен или тысяч текстов, а десятки миллионов. Тут вы понимаете, что для решения задачи вам не хватит одной (даже очень мощной машины) хотя бы потому, что размер обрабатываемых данных не поместится ни на один диск! Именно с такой проблемой впервые столкнулись в компании Google в начале нулевых. Там же было придумано решение которое они называли [MapReduce](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Постановка задачи и требования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, вам нужно написать систему которая позволяла бы обрабатывать действительно огромные объемы данных. Естественно, что она должна уметь решать не только нашу задачу про подсчет слов, но и достаточно широкий класс других задач.\n",
    "\n",
    "Ограничение от которого вы не сможете избавиться -- это хранение огромного массива данных. Так как все эти данные не поместятся на одну машину, то вам придется использовать несколько. Совокупность всех этих машин называют *кластером*, а сами машины *нодами*. Необходимость работы с несколькими машинами создает множество проблем. А именно, вам понадобятся:\n",
    "\n",
    " - **Распределенная файловая система.** Именно в ней будут хранится все массивы данных с которыми вы собрались работать.\n",
    " - **Модель вычислений.** Работа с кластером должна происходит через такой фреймворк/модель вычислений чтобы пользователи могли легко писать распараллеливаемые программы. Без параллелизации код пользователей не сможет полностью задействовать ресурсы кластера.\n",
    " - **Контроль исполнения задач.** На кластере будут запускаться задачи разных пользователей . Нужна система диспетчеризации и распараллеливания.\n",
    " - **Система управления ресурсами.** Разные задачи требуют разных ресурсов. Чьи-то задачи требуют много процессорного времени, а кто-то просто преобразует огромный массив данных в другой формат. Нужна система для оптимального распределения процессоров, жестких дисков, оперативной памяти.\n",
    " \n",
    "При построении всех этих компонент надо учитывать важную особенность - в больших кластерах всегда ломается какая-нибудь машина, а может и несколько. Причин может быть много - сломалась или тормозит сеть, вышел из строя диск, пользовательская операция исчерпала все ресурсы или вошла в бесконечный цикл, машину отключили для замены оборудования. Кластер должен сам решать эти проблемы или минимизировать участие пользователя в их решении. Такое требование называется *fault tolerance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Распределенная файловая система"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Интерфейс\n",
    "\n",
    "Задача распределенной файловой системы дать пользователю удобную абстракцию, в которой он мог работать именно с файлами и не задумываться о деталях реализации. Естественной и привычной абстракцией была бы файловая система локальных компьютеров. Другими словами, мы хотим\n",
    " - Организацию файлов в по директориям\n",
    " - Возможность обращатся к файлам через пути. Например,\n",
    " ```\n",
    " ls //home/my_dir_with_big_data\n",
    " ```\n",
    " - Возможность создания, копирования и переноса директорий и файлов. Например,\n",
    " ```\n",
    " cp //home/my_dir1/big_table //home/my_dir2/big_table_copy\n",
    " ```\n",
    " - Возможность конкатенации файлов и приписывания к ним новых данных\n",
    " - Возможность получения метаинформации о файлах. Например размер, права доступа, время создания и модфикации.\n",
    "\n",
    "Конечно, давать пользователю весь интерфейс обычной файловой системы здесь не получится хотя бы потому, что некоторые операции (например, запись в произвольное место файла) сложно реализовать эффективно в распределенной файловой системе.\n",
    "\n",
    "#### Чанки\n",
    "\n",
    "Работа с данными распределенными по нескольким машинам значительно отличается от работы на локальной файловой системе. Файлы которые мы будем хранить, наверняка не поместятся на одной машине. Решение здесь достаточно простое - мы разбиваем такие файлы на чанки (*chunks*) и храним на машинах именно их. Оптимальный размер чанка обычно несколько десятков или сотен мегабайт. Так как безотказных машин не бывает, то нельзя хранить на кластере чанки в единственном экземпляре. Каждый чанк надо продублировать на нескольких других машинах. Количество копий называется *коэффициентом репликации*. Из-за того, что данные хранятся в избыточном виде, а жесткие диски стоят денег, то чанки надо уметь архивировать. Для данных к которым редко обращаются можно использовать алгоритмы с сильным коэффициентом сжатия. \n",
    "\n",
    "Дупликация данных имеет и недостатки. Например, если вы захотите изменить содержимое одного чанка, то вы должны сделать это атомарно и со всеми его копиями. Это сложная задача, поэтому в первых реализациях распределнных файловых систем не давали такой возможности, а разрешали лишь добавлять новые чанки. Получается, в общем случае, для изменения файла вам надо его полность создать заново.\n",
    "\n",
    "#### Метаданные\n",
    "\n",
    "Как и в любой файловой системе надо уметь хранить не только сами данные, но и дополнительную информацию об этих данных. Например,\n",
    "\n",
    " - для каждого файла мы должны знать все его чанки \n",
    " - для каждого чанка надо знать машины на которых он лежит\n",
    " - для каждой машины надо знать какие чанки на ней хранились, (на случай отказа машины)\n",
    " - для каждого файла мы должны знать его права доступа, время создания и прочую служебную информацию\n",
    " \n",
    "Все это вместе называют метаданными. Они не занимают много места и поэтому их удобно хранить на одной машине которую называют *мастером*. Он занимается хранением и атомарным обновлением и получением метаинформации о файлах кластера. Вся метаинформация хранится в оперативной памяти мастера. Отметим, что сам мастер работает только с метаданными. Когда пользователи хотят прочитать или записать в файлы, то пересылка данных происходит между пользователем и машинами кластера, но по особому протоколу в котором участвует мастер. Мастер следит за консистентностью метаданных и успешностью операции записи/чтения.\n",
    "\n",
    "#### Отказоустойчивость\n",
    "\n",
    "Для поддержания консистентности мастер должен знать какие машины кластера в рабочем состоянии. Для этого он ожидает, что машины регулярно с постоянным интервалом времени будут отправлять мастеру сообщения о своем состоянии (так называемые *heartbeat*). Если от какой-то машины не приходит heartbeat, то она помечается как сломанная для чанков которые хранились на ней запускается процесс повышения коэффициента репликации.\n",
    "\n",
    "Слабым местом во всей этой схеме есть и остается мастер. Если он сломается или потеряет свое состояние, то весь кластер будет в нерабочем состоянии. Поэтому регулярно требуется делать бэкапы состояния мастера. Для большей надежности рядом с мастером держат еще одну или несколько машин (*вторичные мастера*), которые сохраняют у себя состояние мастера и включаются в работу если отказал основной мастер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cluster](cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Модель вычислений Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки больших объемов данных необходимо использовать параллелизацию. Алгоритмы которые можно разбить на подзадачи выполняемые одновременно на нескольких вычислителях называют конкурентными. Написание таких алгоритмов довольно сложное занятие, поэтому нужно придумать фреймворк/библиотеку/модель вычислений которая позволяла бы писать легко распараллеливаемые алгоритмы. Такой моделью вычислений оказалась модель Map Reduce о которой мы сейчас поговорим.\n",
    "\n",
    "Map и reduce это функции высшего порядка пришедшие из функциональных языков. Известно что программы написанные в функциональном стиле хорошо распараллеливаются. Если программисты будут писать свои алгоритмы в терминах функций map и reduce, то их программы будут автоматически обладать конкурентностью и задачу параллелизации можно будет переложить с плеч программистов на фреймворк в рамках которого они будут писать код. Разберем как устроены эти функции.\n",
    "\n",
    "### Функция map\n",
    "\n",
    "Функция map имеет два аргумента\n",
    " - Первый аргумент -- пользовательская функция $f:U\\to List[V]$. Она получает на вход один объект типа $U$ и генерирует список элементов типа $V$\n",
    " - Второй аргумент -- список $L$ типа $List[U]$. \n",
    " \n",
    "Далее map поэлементно применяет функцию $f$ (именно тут происходит распараллеливание) к элментам списка $L$. Получающиеся результаты от каждого применения объединяются в один список типа $List[V]$.\n",
    "\n",
    "### Функция reduce\n",
    "\n",
    "Функция reduce имеет два аргумента\n",
    " - Первый аргумент - функция $g:Pair[K, List[V]]\\to List[W]$. Она получает на вход пару из ключа типа $K$ и списка  значений типа $V$ и генерирует список элементов типа $W$.\n",
    " - Второй аргумент - список $L$ типа $List[Pair[K, List[V]]]$ состоящий из пар вида: ключ и его список значений.\n",
    " \n",
    "Далее reduce применяет пользовательскую функцию $g$ (именно тут происходит распараллеливание) к элментам списка $L$. Получающиеся результаты от каждого применения объединяются в один список типа $List[W]$. \n",
    "\n",
    "### Протокол\n",
    "\n",
    "На первый взгляд функция reduce является частным случаем функции map, но их стоит рассматривать как разные, потому что у них следюущий протокол работы.\n",
    "\n",
    " 1. Фаза **map** применяет пользовательскую функцию $f$ к элементам $u$ списка $L$ и получает список  $L'$\n",
    " 2. Фаза **shuffle** для каждого элемента $v$ списка $L'$ назначается ключ $k$. Обычно этот ключ - часть данных хранящися в самом элементе $v$. Элементы списка $L'$ имеющие один и тот же ключ собираются в одну группу и формируется пара $(k, [v_1,\\ldots,v_{n(k)}])$. Список таких пар обозначим $L''$.\n",
    " 4. Фаза **reduce** применяет пользовательскую функцию $g$ к парам из списка  $L''$ и получает результирующий список $L'''$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mapreduce](mapreduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теоретически доказано, что подобная модель вычислений Тьюринг полная. Другими словами на ней можно реализовать любой алгоритм, но не факт что эффективно.\n",
    "\n",
    "Теперь код для фаз map и reduce задачи подсчета слов на гипотетическом фреймфорке для MapReduce мог бы выглядеть приблизительно так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(record):\n",
    "    for word in extract_words(record['text']):\n",
    "        yield {'word': word, 'count': 1}\n",
    "\n",
    "def g(key, records):\n",
    "    yield {'word': key, 'total': sum(record['count'] for record in records)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Детали реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В теории все легко и просто, но на практике появляется много нюансов о которых мы сейчас поговорим. \n",
    "\n",
    "В общем случае пользовательская операция может содержать только map фазу или только reduce фазу либо обе. Как мы обсудили выше данные операции легко распараллеливаются на более мелкие подзадачи. Эти подзадачи мы будем называть джобами (*jobs*). По сути джоба это процесс запускаемый на машине кластера.\n",
    "\n",
    "#### Чтение входа и запись выхода операции\n",
    "\n",
    "Для каждой из рассматриваемых операций на входе и на выходе находятся файлы. Модель вычислений предполагает, что входные данные состоят из элементов одинакового типа. Поэтому чтение и запись выглядят немного сложнее чем думалось.\n",
    "Весь процесс выглядит так\n",
    "\n",
    "1. На входе каждой операции должен стоять читатель данных (InputFormat), который обрабатывает чанки файла и разбивает его на части называемые сплитами (*splits*). Тут конечно могут возникнуть проблемы с несвпадением границ сплитов и чанков, но это легко решаемая задача. \n",
    "\n",
    "2. Далее каждый сплит сериализуется и отправляется по сети на свою машину и там запускается отдельная джоба. \n",
    "\n",
    "3. В джобе вычитывается сплит и разбивается на записи (*records*) c помощью *RecordsReader*, которые отправляются в пользовательскую функцию.  После того как пользовательская функция закончила обработку она возвращает другие записи. Эти записи накапливаются в памяти процесса в некотором буфере и при переполнении данные сбрасываются из буфера на диск. При сбросе запускается код (*OutputFormat* и *OutputReader*) преобразующий записи в выходной формат. Принцип их работы симметричен работе *InputFormat* и *InputReader*.\n",
    "\n",
    "4. Если выход джобы не помещается на самой машине, то результат пересылается по сети на другие.\n",
    "\n",
    "#### Работа джобы для map фазы\n",
    "\n",
    "Мы уже обсудили, что вход для оперции формируется в виде записей. В случае map операции в джобе запускается простой цикл который применяет пользовательскую функцию к записям и возвращает результат. В случае map_reduce операции записи, если их не очень много, не складываются на диск а сразу из памяти джобы отправляются на шаг shuffling.\n",
    "\n",
    "#### Организация shuffling фазы\n",
    "\n",
    "Это самая тяжелая стадия map_reduce операций, потому что здесь будут задействованы сортировки и что еще хуже - передача данных по сети. Эта фаза начинается когда отработают все джобы родительской map фазы. Для работы shuffling фазы пользователь должен указать количество reduce джобов $M$ которые он будет в дальнейшем запускать и код (*Partitioner*) который умеет по ключу вычислять номер reduce джобы в которую отправятся данные этого ключа. Далее работа просходит следующим образом:\n",
    "\n",
    "1. В рамках одной джобы фазы map из каждой записи $v$ мы достаем ключ $k$, далее *Partitioner* выдает номер $p$ reduce джобы в которую будет отправлена пара $(k,v)$. Совокупность пар отправляющихся в одну reduce джобу называется партицией (*partion*). По умолчанию номер партиции вычисляется как остаток от деления некоторого хэша ключа $k$ на количество reduce джобов. Трюк с вычислением $p$ нужен для того чтобы равномерно распределить выход map фазы по reduce джобам.\n",
    "\n",
    "2. Далее, в рамках одной джобы все тройки $(p, k, v)$ сортируются по $(p, k)$.\n",
    "\n",
    "3. Теперь со всех машин на которых идет shuffling нашей оперции начинают пересылаться записи $(p, k, v)$. Машина назначения вычисляется по номеру $p$. Каждая машина назначения делает merge sort по ключу k приходящих потоков данных. Этот процесс отработает корректно, потому что входные данные были отсортированы по $(p, k)$.\n",
    "\n",
    "#### Работа джобы для  reduce фазы\n",
    "\n",
    "После фазы shuffle на входе у reduce джобы будет набор чанков в которых данные организованы так что записи с одним ключом идут подряд. Reduce джоба читает каждый такой диапазон записей и запускает пользовательскую функцию $g$. Результат записывается на диск с использованием *OutputFormat* и *OutputReader*.\n",
    "\n",
    "#### Подводим итог\n",
    "\n",
    "Как видно для map_reduce операции нам потребуется\n",
    "\n",
    " - Код для чтения данных (*InputFormat*)\n",
    " - Код для map фазы (*Mapper*) \n",
    " - Код для reduce фазы (*Reducer*) \n",
    " - Количество reduce джоб $M$\n",
    " - Код для разбиения выхода маппера на партиции (*Partitioner*)\n",
    " - Код для записи результата (*OutputFormat*)\n",
    "\n",
    "Это позволяет достаточно тонко настроить и оптимизировать операцию, особенно если вы запускаете ее регулярно. На практике обычно\n",
    "\n",
    " - Используются дефолтные форматы входа и выхода, и эти форматы обычно совпадают\n",
    " - Количество необходимых reduce джобов вычисляется эвристически\n",
    " - Используется дефолтный *Partitioner*\n",
    "\n",
    "Таким образом от пользователя в стандартной ситуации требуется только необходымий минимум - код для map и reduce фаз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Контроль исполнения задач и управление реурсами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь когда мы разобрали в деталях работу операций поговорим о том как ее инциализировать и провести контроль исполнения. Для этих задач на мастере есть еще одна система. Будем называть ее шедулер (*scheduler*). Она  \n",
    " - Следит за доступностью машин\n",
    " - Следит за распределением вычислительных ресурсов на кластере\n",
    " - Запускает операции и следит за их выполненеим.\n",
    " \n",
    "### Контроль исполнения\n",
    " \n",
    "Разберем как идет работа для map_reduce операции:\n",
    "\n",
    "1. Пользователь отправляет на кластер запрос на запуск операции\n",
    "2. Шедулер создает на одной из свободных и доступных машин процесс аппмастер (*application master*). Этот процесс будет следить за исполнением операции и запрашивать ресурсы у шедулера.\n",
    "3. Аппмастер скачивает себе пользовательский код в виде (.jar архивов если это java или .pyc файлов если это питон)\n",
    "4. Аппмастер сообщает шедулеру пути к файлам на которых он хочет запустить map фазу. Также он сообщает какие ему нужны для этого ресурсы.\n",
    "5. Шедулер пытается выделить запрашиваемые ресурсы на тех машинах где хранятся чанки запрашиваемых файлов, чтобы избежать лишней пересылки данных. Если это не удается то ресурсы выделяются где придется.\n",
    "6. Аппмастер запускает джобы map фазы. Обычно это контейнеры (типа docker контейнеров), в которых будет работать код фазы и пользовательской функции. \n",
    "7. Аппмастер оценивает количество необходимых reduce джобов (или использует заданное значение) и запрашивает у шедулера машины для сохранения результатов shuffle фазы.\n",
    "8. Шедулер выдает ему список машин.\n",
    "11. Аппмастер запускает shuffle фазу, а потом reduce джобы.\n",
    "12. После завершения операции управление возвращается в пользовательский код\n",
    "\n",
    "### Управление ресурсами\n",
    "\n",
    "Операции кластера используют дисковое пространство, вычислительные процессоры, оперативную память и сеть. Распределением этих ресурсов занимается шедулер. Он, зная статистику уже используемых ресурсов, пытается для вновь пришедших задач  распределить оставшиеся ресурсы наиболее честным образом. Алгоритм зависит от используемого понятия честности. Один из первых подходов - одинаковое распределение ресурсов между процессами. Но если один пользователь создаст много операций, то получит бОльшую часть вычислительных ресурсов кластера. Поэтому предпочтительнее распределение ресурсов по пользователям и группам пользователей. Для этой задачи есть алгоритм [fair share](https://en.wikipedia.org/wiki/Fair-share_scheduling).\n",
    "\n",
    "### Обработка ошибок\n",
    "\n",
    "Шедулер следит за доступностью машин. Если одна из машин с джобой упала, то шедулер и сообщит об этом аппмастеру. Аппмастер перезапустит джобу. Но тут есть вопрос какую. Если упала джоба map фазы то достаточно перезапустить только ее. Если упала джоба reduce фазы, то перезапускается вся операция так как вход reduce джобы может зависеть от выхода всех мапперов. Казалось бы этого можно избежать, сохраняя промежуточные результаты, но практика показывает, что проще перезапускать упавшие джобы/операции чем хранить промежуточные результаты и тратить дисковые ресурсы кластера."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
